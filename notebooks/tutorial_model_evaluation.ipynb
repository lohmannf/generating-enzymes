{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Total Variation Distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One metric to compare the performance of different models is to determine the similarity of each generated sequence to some reference (e.g. wildtype reference or closest sequence in training dataset), which yields a histogram of similarity values for each model. The total variation distance (TVD) between the histograms of different models can then be calculated as a notion of model similarity. When using the wildtype sequence as reference for the similarity, models can even be compared to the training dataset.\n",
    "\n",
    "Running the TVD computation is implemented in ```scripts/tvd_matrix.py```. Computation for the MID1 and IRed datasets is supported out of the box, simply modify the paths in ```res_dirs``` at the bottom to point to the files containing the generated sequences of your models.\n",
    "When adding your own dataset, note that the script assumes that the loader class implements the ```loader.reference``` attribute which contains a string with the wildtype reference sequence.\n",
    "\n",
    "If the obtained values in the tvd matrix exhibit odd behavior (e.g. all very small / large), one potential error source might be the choice of bins for determining the histograms. For the linear metrics, Blosum and One-hot similarity, ```bins``` specifies the number of equally spaced bins to choose. For the inverse metrics (Hamming similarity and Blosum-weighted Hamming similarity), it specifies the distance in terms of (weighted) insertions between two bin edges. The bin interval will always range from smallest to highest similarity value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 ../scripts/tvd_matrix.py --use_wildtype -l 290 -d ired -o ./tvd_matrix.png"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESM-2 latent space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embeddings of generated and real sequences obtained from Evolutionary Scale Modelling 2 (ESM-2) can be used to assess the relative performance of different models. Additionally, the perplexity of ESM-2 can be employed as a proxy for biological feasibility of the sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to specify a handle for each model and a path to where the sequences generated by it are stored. This is done as a ```dict``` like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_dirs = {\n",
    "    \"sedd\": \"./gen_data/sedd/ired.fasta\",\n",
    "    \"dfm\": \"./gen_data/dfm/ired.fasta\",\n",
    "    \"frozen\": \"./gen_data/frozen/1.5.1.-.fasta\",\n",
    "    \"pretrained\": \"./gen_data/zymCTRL_results/1.5.1.-.fasta\",\n",
    "    \"tiny\": \"./gen_data/tiny_model/1.5.1.-.fasta\",\n",
    "    \"small\": \"./gen_data/small_model/1.5.1.-.fasta\",\n",
    "    \"ft lr 8e-05\": \"./gen_data/zytune_untrunc/1.5.1.-.fasta\",\n",
    "    \"ft lr 8e-06\": \"./gen_data/zymctrl_lr-06/1.5.1.-.fasta\",\n",
    "    \"ft lr 8e-07\": \"./gen_data/zymctrl_lr-07/1.5.1.-.fasta\",\n",
    "    \"ft lr 8e-08\": \"./gen_data/zymctrl_lr-08/1.5.1.-.fasta\",\n",
    "    \"ft lr 8e-09\": \"./gen_data/zymctrl_lr-09/1.5.1.-.fasta\",\n",
    "    \"potts gwg\": \"./gen_data/potts_adam_gwg_T_0.01_lr_0.001_unified.fasta\",\n",
    "    \"random informed\": \"./gen_data/random_trained.fasta\",\n",
    "    \"random\": \"./gen_data/random.fasta\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, the data has to be loaded and ```n_per_dataset``` unique sequences can be sampled randomly.\n",
    "In order to prevent tokenization issues with the ESM-2 tokenizer, we remove any numbers and other unknown characters from the sequences. Depending on whether you choose to extend the alphabet of some models with additional characters that are not known by the ESM-2 tokenizer, you would have to remove these aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from genzyme.data import loaderFactory\n",
    "from genzyme.evaluation.embedding import ESM, EmbeddingStats\n",
    "\n",
    "n_per_dataset = 600\n",
    "l = 97\n",
    "seed = 7\n",
    "\n",
    "seqs = []\n",
    "labs = []\n",
    "\n",
    "np.random.seed(seed)\n",
    "\n",
    "for i, path in enumerate(res_dirs.values()):\n",
    "    with open(path, \"r\") as file:\n",
    "        lines = [s.strip().replace(\"[UNK]\", \"\").replace(\"X\", \"\") for s in file.readlines() if not s.startswith('>')]\n",
    "        lines = np.unique([''.join([pos for pos in seq if not pos.isdigit()]) for seq in lines])\n",
    "        \n",
    "    lines = np.unique([x[:l] for x in lines if len(x) >= l])\n",
    "\n",
    "    if len(lines) > n_per_dataset:\n",
    "        lines = np.random.choice(lines, n_per_dataset, replace=False)\n",
    "\n",
    "    seqs.extend(lines)\n",
    "    labs.extend(len(lines)* [i])\n",
    "\n",
    "train_dat = loaderFactory()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, the training data can be loaded aswell using a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dat = loaderFactory()\n",
    "train_dat.load('ired')\n",
    "train_dat._replace(\"*\")\n",
    "\n",
    "lines = train_dat.get_data()[np.random.choice(len(lines), n_per_dataset, replace=False)]\n",
    "labs.extend([i+1]*len(lines))\n",
    "seqs.extend(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the embeddings, negative log-likelihoods and perplexities using the ESM-2 model. As the model can handle sequences of different lengths, there's no need to perform truncation / filtering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embs = ESM()\n",
    "embeddings, nll, ppl = embs.get_embeddings(seqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Average perplexity and NLL per model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_nll = {}\n",
    "avg_ppl = {}\n",
    "for l in np.unique(labs):\n",
    "    avg_nll[(list(res_dirs.keys())+[\"train data\"])[l]] = np.mean(nll[np.array(labs) == l])\n",
    "    avg_ppl[(list(res_dirs.keys())+[\"train data\"])[l]] = np.mean(ppl[np.array(labs) == l])\n",
    "\n",
    "print(avg_nll)\n",
    "print(avg_ppl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = EmbeddingStats(np.array(labs), np.array(embeddings), label_names = list(res_dirs.keys())+[\"train data\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the average silhouette score of the clusters defined by the model membership"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.silhouette()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the embeddings using a PCA or UMAP projection. The default is a PCA projection colored by model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "_, ax = stats.plot_embeddings()\n",
    "ax.set_title('ESM-2 embeddings (PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "_, ax = stats.plot_embeddings(color = ppl, col_name = \"perplexity\")\n",
    "ax.set_title('ESM-2 embeddings (PCA)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = stats.plot_embeddings(umap=True)\n",
    "ax.set_title('ESM-2 embeddings (UMAP)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = stats.plot_embeddings(color = ppl, col_name = \"perplexity\", umap=True)\n",
    "ax.set_title('ESM-2 embeddings (UMAP)')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

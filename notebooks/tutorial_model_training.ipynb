{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The models and evaluation are built based on the custom DataLoader class.\n",
    "Every model implements its own subclass of the base DataLoader which handles custom data preprocessing and can process additional information written to the generated fasta files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding new datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To add a new dataset that can be processed with the loaders, a new function ```loader._load_<dataset>(...)``` has to be implemented. This function should handle loading of the sequences as a 1D-array of strings into the ```loader.data``` attribute and setting the ```loader.reference```attribute with the wildtype reference sequence of the respective dataset (otherwise performing the total variation distance computation with respect to the wildtype will not work). Additional custom attributes (e.g. fitness) can also be instantiated in this function. A new case for the dataset to be added to ```loader.load``` which defines the short name of the dataset and calls the specific load function implemented above. \n",
    "Now, the new dataset can be loaded as follows and used with the models below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genzyme.data import loaderFactory\n",
    "dataset_name = \"ired\" # put the name of the new dataset as defined in .load here\n",
    "loader = loaderFactory()\n",
    "loader.load(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains example code for training the models and generating new sequences with them. The hyperparameters for each model as well as preprocessing, training and generation are handled in a ```.yaml``` file stored in the respective subdirectory of ```../configs```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import os\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from genzyme.models import modelFactory\n",
    "from genzyme.data import loaderFactory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ZymCTRL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(os.path.join(os.path.dirname(__file__), \"../configs/zymctrl/config.yaml\"))\n",
    "OmegaConf.resolve(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ZymCTRL model can be run with 3 different architectures that differ in the number of attention heads and layers:\n",
    "- Original (20 heads, 36 layers)\n",
    "- Small (10 heads, 20 layers)\n",
    "- Tiny (5 heads, 5 layers)\n",
    "\n",
    "Since the provided model class is merely a wrapper for the huggingface models, the config for the latter is provided separately in the ```model_dir```. It can point to the huggingface hub path in the case of the original model and has to point to a local directory containing the tokenizer and model configs for the custom smaller models (default is ```./../../data/ZymCTRL_<architecture>```). Note that the path must be specified relative to ```zymctrl.py```if it's not absolute. A mapping of the model names to the ```model_dir``` can be found below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cfg.model.name == \"zymctrl\":\n",
    "    cfg.model.dir = \"AI4PD/ZymCTRL\"\n",
    "\n",
    "elif cfg.model.name == \"small\":\n",
    "    cfg.model.dir = './../../data/ZymCTRL_small'\n",
    "\n",
    "elif cfg.model.name == \"tiny\":\n",
    "    cfg.model.dir ='./../../data/ZymCTRL_tiny'\n",
    "\n",
    "elif cfg.model.name != \"zymctrl\":\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To preprocess the desired dataset, the respective loader class is used. It handles proper batching and tokenization of the sequences and flushes the train and test datasets to disk at the specified ```data_dir```. The datasets can also be kept in memory and returned directly by setting ```save=False```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from genzyme.models.utils import SpecialTokens\n",
    "\n",
    "if cfg.data.reload:\n",
    "    loader = loaderFactory(\"ctrl\")\n",
    "    loader.load(cfg.data.name)\n",
    "    loader.assign_control_tags(cfg.data.tag)\n",
    "    loader.set_tokenizer(AutoTokenizer.from_pretrained(cfg.model.dir))\n",
    "\n",
    "    special = SpecialTokens(\"<start>\", \"<end>\", \"<pad>\", \"<|endoftext|>\", \"<sep>\", \" \")\n",
    "    loader.preprocess(special, cfg.data.test_split, 0, save=True, data_dir=f\"../datasets/{cfg.data.name}/\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelFactory(\"zymctrl\", cfg = cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For training, only the train and validation dataset have to be specified, all other hyperparameters are passed via ```cfg```. One can either pass the datasets directly or specify the path to their location on disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_training(train_dataset = f'../datasets/{cfg.data.name}/train',\n",
    "                   eval_dataset = f'../datasets/{cfg.data.name}/test',\n",
    "                   cfg = cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A list of EC numbers can be specified, for each of which ```cfg.generation.n_seqs``` sequences will be generated. The sequences of all prompts will be written to a fasta file whose path can be specified in ```cfg``` with the headers containing the prompt and the model perplexity for each sequence. If ```cfg.generation.keep_in_memory=True```, the sequences will also be returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SEDD & Discrete Flow Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Score Entropy Discrete Diffusion (SEDD) and Discrete Flow Modeling (DFM) are implemented in the same model class and can be switched between by changing the loss and noise type attributes in the config as follows:\n",
    "\n",
    "| Model | ```cfg.training.loss``` | ```cfg.noise.type``` |\n",
    "| ----- | ----------------- | -------------- |\n",
    "| SEDD | ```\"dwdse\"``` | ```\"loglinear\"``` |\n",
    "| DFM | ```\"ce\"``` | ```\"linear\"``` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelFactory(\"sedd\", cfg_dir = \"./../../configs/sedd/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model uses the config stored at ```config_dir``` as default. Any runtime overrides to the default config can be passed directly as a config object that only contains the relevant attributes that are to be overriden.\n",
    "Data loading and preprocessing is handled by the ```run_training``` method.\n",
    "\n",
    "The config that was used for training is stored in the ```cfg.work_dir```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_overrides = OmegaConf.create({\"data\": {\"name\": \"ired\", \"test_split\": 0.2, \"grouped\": False}, \n",
    "                                    \"training\": {\"n_iters\": 100000, \"distributed\": True, \"loss\": \"ce\", \"batch_size\": 128},\n",
    "                                    \"optim\": {\"lr\": 3e-5},\n",
    "                                    \"noise\": {\"type\": \"linear\"},\n",
    "                                    \"eval\": {\"batch_size\": 64},\n",
    "                                    \"model\": {\"length\": 291}})\n",
    "model.run_training(train_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The generation function takes as argument the path to the directory that contains the training config. If no particular checkpoint is specified explicitly, the model will try to load the most recent checkpoint it can find in ```model_path/checkpoints```.\n",
    "\n",
    "Similar to the training, the generate method also allows for overrides of config arguments by passing a config object with the relevant attributes.\n",
    "\n",
    "Most importantly, the predictors are specific to sedd or dfm and should only be used as follows:\n",
    "\n",
    "| Model | ```cfg.sampling.predictor``` |\n",
    "| ----- | ----------------- |\n",
    "| SEDD | ```\"euler\"``` or ```\"analytic\"``` |\n",
    "| DFM | ```\"euler-dfm\"``` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"<YOUR WORK DIR>\"\n",
    "gen_overrides = OmegaConf.create({\"sampling\" : {\"batch_size\": 32, \"steps\": 1000, \"n_samples\": 10000, \"predictor\": \"euler-dfm\"},\n",
    "                                      \"out_dir\": \"../gen_data/dfm/\"})\n",
    "model.generate(model_path, gen_overrides)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Energy-based Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The abstract baseclass ```EnergyBasedModel``` implements core functions and an MCMC sampling routine. It has two child classes, ```DeepEBM``` and ```PottsModel```, that each implement their own training methods. They are both derived from ```torch.nn.Module``` aswell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Potts Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/potts/config.yaml\")\n",
    "OmegaConf.resolve(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The preprocessing is handled by the Potts loader class. The preprocess method returns pytorch dataloaders for training and testing. When using ```cfg.optimizer.method=\"l-bfgs\"```, make sure to set the ```train_batch_size```equal to the training set size because the model will only be trained on the first batch from the loader.\n",
    "\n",
    "Note that this implementation of energy-based models always expect fixed-size input (due to the fixed size of the parameters and because only fixed-length sequnences can be sampled at generation time) and therefore the training data has to be truncated / filtered to the same length. This can be achieved with ```loader.unify_seq_len(<LENGTH>)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loaderFactory(\"potts\")\n",
    "loader.load(cfg.data.name)\n",
    "loader.unify_seq_len(cfg.model.L)\n",
    "train_data, test_data = loader.preprocess(cfg.training.test_split, \n",
    "                                          train_batch_size=cfg.trainig.batch_size, \n",
    "                                          test_batch_size=cfg.training.batch_size, \n",
    "                                          d = cfg.model.d, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = modelFactory(\"potts\", cfg = cfg)\n",
    "model.run_training(train_data, test_data, cfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"your/model/name.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To extract a contact map from the potts model, run ```get_coupling_info```. The contact map can be generated with or without performing average product correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "contacts = model.get_coupling_info(apc=True)\n",
    "fig, ax = plt.subplots()\n",
    "ax.imshow(contacts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to start a run of the Markov chain, the starting state has to be provided. This can be a random sequence generated via ```get_random_seq``` or any arbitrary sequence in one-hot encoding with ```model.d```classes.\n",
    "The model can be re-seeded via ```model.set_seed(<SEED>)```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = model.get_random_seq()\n",
    "x0 = torch.nn.functional.one_hot(x0, num_classes = model.d)\n",
    "model.generate(cfg, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep Energy-based Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The deep energy-based model can be run with two different implementations of the energy function:\n",
    "1. Energy modelled by ESM-2 with finetuned head\n",
    "2. Energy modelled as quadratic form of pretrained ESM-2 embeddings with learnable matrix\n",
    "\n",
    "where $s$ indicates the ESM-2 model and subscript $\\theta$ indicates that a component is trainable.\n",
    "\n",
    "The different models can be specified as follows\n",
    "| Model | ```cfg.model.em_name```|\n",
    "| -- | ---------------------- |\n",
    "| $f_\\theta(x) = s_\\theta(x)$ | ```\"esm\"``` |\n",
    "| $f_\\theta(x) = s(x)^T A_\\theta s(x)$ | ```\"quadratic\"``` |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = OmegaConf.load(\"../configs/deep_ebm/config.yaml\")\n",
    "OmegaConf.resolve(cfg)\n",
    "model = modelFactory(\"debm\", cfg = cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the training and validation sequences need to have the same length. Because the ESM-2 model, which is used to model the energy, takes tokenized sequences where amino acids are represented as integers as input, the ESM-2 tokenizer has to be passed to the preprocessing function.\n",
    "The resulting dataloaders contain pairs of tokenized sequences, one encoded with the ESM-2 tokenizer, the other encoded with a simpler integer encoding with ```model.d``` classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loaderFactory(\"debm\")\n",
    "loader.load(\"ired\")\n",
    "loader.unify_seq_len(cfg.model.L)\n",
    "train_dl, test_dl = loader.preprocess(cfg.training.split,\n",
    "                                      cfg.training.batch_size,\n",
    "                                      cfg.training.val_batch_size,\n",
    "                                      tokenizer = model.em_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.run_training(train_dl, test_dl, cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x0 = model.get_random_seq()\n",
    "x0 = torch.nn.functional.one_hot(x0, num_classes = model.d).double()\n",
    "model.generate(cfg, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random model is the only model that does not use a config file (due to its simplicity). It will generate randomly drawn amino acid sequences of a fixed length. The model can be run in two modes:\n",
    "- Without training: The model will draw an amino acid uniformly at random at each position\n",
    "- With training: The model will draw an amino acid from the maximum likelihood estimate of the categorical distribution at each position (conserves the marginals of the amino acids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The random model can be trained either on fixed length sequences or on sequences of different lengths by padding sequences that are shorter. The model comes with a built in function that handles padding to the desired maximum length. Preprocessing is only necessary when running the model with training, otherwise it suffices to pass the desired length of the resulting sequences to the constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "loader = loaderFactory(\"random\")\n",
    "loader.load(\"ired\")\n",
    "max_len = np.max([len(seq) for seq in loader.get_data()])\n",
    "\n",
    "model = modelFactory(\"random\", length = max_len)\n",
    "loader.set_data(model.pad_data(loader.get_data()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, the sequences can be truncated/filtered to the same length with the built in method from the dataloader instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = loaderFactory(\"random\")\n",
    "loader.load(\"ired\")\n",
    "loader.unify_seq_len(290)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training is optional, if omitted the model will generate sequences drawn uniformly at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = loader.get_data()\n",
    "l = len(train_data[0])\n",
    "model = modelFactory(\"random\", max_len = l)\n",
    "model.run_training(train_dataset = train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.generate(n_samples = 10000, output_file = \"sequences.fasta\", keep_in_memory = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

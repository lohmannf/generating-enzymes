model:
  dir: AI4PD/ZymCTRL #hf hub repo or local directory
  seed: 31 # random seed
  name: zymctrl # identification name of the model

data:
  name: mid1 #dataset name, e.g. ired or mid1
  tag: 3.1.-.- # tag that is used to prepend the training data
  reload: true # whether the dataset should be reloaded and reprocessed. set false if dataset is already stored on disk
  test_split: 0.1 # fraction of data to use for validation

training:
  n_epochs: 10 # max number of epochs
  freeze: true # freeze model except the LM head
  train_batch_size: 1 # effective batch size is bigger because the context length is 1048
  eval_batch_size: 4
  log_freq: 5 
  log_level: info
  eval_freq: 10
  snapshot_freq: 100
  snapshot_limit: 2
  cache_dir: "."
  ckpt: null
  ckpt_dir: null
  sample: false #perform snapshot sampling
  snapshot_sampling:
    freq: 100
    batch_size: 20
    output_file: ${...training.work_dir}/train_generation.fasta
    prompt: ${...data.tag}
    kwargs: ${...generation.kwargs}

  use_wandb: true
  wandb: # wandb.init kwargs
    project: zymctrl

  optimizer:
    lr: 8e-05

  work_dir: /cluster/project/krause/flohmann/zymctrl/${..model.name}_f_${..training.freeze}_lr_${..training.optimizer.lr}

generation:
  prompts:
    - 3.1.-.-
  n_seqs: 10000
  batch_size: 20
  output_file: ${..training.work_dir}/zymctrl_f_${..training.freeze}_lr_${..training.optimizer.lr}.fasta
  keep_in_memory: false
  kwargs:
    top_k: 9
    repetition_penalty: 1.2
    max_length: 1024
    eos_token_id: 1
    pad_token_id: 0
    do_sample: true 
